강화학습 활용 사례
    감정, 관절, 에너지절약, 상품추천, 맞춤광고

MaxQ = maxQ(state, action)
policy(파이)(state) = argmaxQ(state, action)   # 최대값이 가리키는 액션을 고르는 policy를 뜻한다. argmax는 max의 액션값

나는 S에 있다.
내가 A를 하면 S1에 간다. (A는 4개)
내가 A를 하면 R을 얻는다.
Q(S,A) = R+maxQ(S1,A1)
즉, 현재의 상태에서 행동에 대한 Q값은 행동에 대한 보상값과 행동을 한 상태에서 받을 수 있는 최대값을 뜻한다.
다시말하면 어떤 행동을 했을 때 그 다음행동에서 성공할 확률이 높은 상태가 나올수록 그 어떤 행동의 Q값이 커진다.

★★★ Exploit & Exploration ★★★
★decaying E-greedy, 랜덤으로 action을 할 확률을 점차 줄여가는 알고리즘
for i in range(1000)
    e = 0.1 / (i+1)   # 처음에는 랜덤확률이 높지만 뒤로 갈수록 랜덤확률이 줄어든다.
    if rand < e:
        a = random
    else:
        a = argmax(Q(state, action))

★add random noise - 각 가중치에 랜덤한 수(노이즈)를 더해서 그 중 가장 큰 수를 따라간다. 어쨌든 가장 높은 가중치를 가지고 있는 액션의 확률이 가장 높다
a = argmax(Q(state, action) + random_values)
a = argmax([0.5 0.6 0.3 0.2 0.5] + [0.1 0.2 0.7 0.3 0.1])   # 더해서 가장 높은 수

★decaying E-greedy와 add random noise를 결합해서 사용할 수 있다.(후반으로 갈 수록 영향력을 줄이는 차원에서, 그래도 높은 가중치의 action으로 갈 수 있다.)

★★★ Discounted reward ★★★
★ 지금 당장에 최대한 빨리 받을 수 있는 보상이 좋다! 는 아이디어
★ 미션을 성공(즉 골에 도달! 하면 reward를 얻는다.)
★ Q(s, a) <- r +0.9(감마)maxQ(s1, a1)  # 즉 보상을 얻는 과정이 길 수록 0.9를 곱하는 횟수(*제곱)이 많아져서 q-value가 작아지게 된다.
★ Q̂(s,a)=r+maxQ̂(s',a')


★★★ Nondeterministic worlds! ★★★
★ 진짜 이해가 안간다
★ Q̂(s,a) = (1-α)Q̂(s,a) + α[r+maxQ̂(s',a')]   # 다음 state의 10%만 학습하고 90%는 현재 상태에서 결정